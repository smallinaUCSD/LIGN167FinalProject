Seshagiri (Seshu) Rao Mallina (A16659161),  Sahil Patel (A16499476),  Hunter Cheng (A16640276),  Jung Tzen Liew (A16390733)

# LIGN 167 Final Paper

## Introduction

&ensp; &ensp; &ensp; Our project focuses on making an adaptive interactive quiz application for students in Professor Styler’s LIGN 101. We utilize calls to the GPT-3.5 API to generate questions that will reflect what the user needs. To do this, we utilized a random sample of 30 lines of the transcript of interest that were provided to us, and prompted the API to generate questions based on this information. The code is generalized to easily switch to GPT-4 if desired or change the amount of lines that are sampled from transcripts for token management. The prompting consisted of giving the API the excerpt, what question type and what type of difficulty was needed for a question and its corresponding answer choices. We utilized a Gradio user interface that allows for our interactive quiz to be an app-based demo on a web server.  We would also like to give credit to Gradio user, JohnSmith9982, for the aesthetics of the page using their “small_and_pretty” theme. The interface allows the user to select which lectures to use, how many quiz questions, and the difficulty level, before generating questions. With each question, there are either multiple choice, short answers, or true and false question types. When a user puts their answer in, it will tell you the correctness of the answer and provide feedback if incorrect. At the end of a quiz, a final score will be calculated, and a new quiz can be started. An interactive dashboard with Plotly’s dash can also be executed on a web server to display quiz score history trends and keep track of all the lectures covered.

&ensp; &ensp; &ensp; We also added an instructor version of the app based on this. The instructor interface prompts questions in the same way, but is designed for the instructor to be able to give them to the students. It prompts questions, gives the correct answers and possible answers, and then allows the instructor to either accept or reject the question generated. If accepted, the question would be loaded into a file and published as a quiz text file once the instructor is done generating all the questions. When rejected, the instructor can put in feedback on what they didn’t like, and have the question regenerated according to the feedback inputted. 

## What Worked and What Didn’t

&ensp; &ensp; &ensp; We spent a lot of time with the issue of getting the questions properly prompted. We ran into problems earlier on where the question prompted was extremely general and not very helpful or lecture specific. We started with a multiple choice format, but then expanded to include T/F and short answer questions. Due to Gradio being our 3rd party application of choice we ran into many problems in terms of styling. In addition to the styling, you can notice that there are some API errors and issues. When testing our application using GPT 3.5 API, some API issues that arose consisted of: long 20-30 second load times, “Failed to generate correctly formatted question”, and blank screens with no question visible. Unfortunately, these errors were at the fault of the API’s feedback system in accordance to our prompting. We provided proper workarounds to account for the issues. With long load times, we ask for the user to restart the application, for failed generations and blank screen we ask the user to either restart the application or press next question and we will not penalize the user for API errors.

&ensp; &ensp; &ensp; Prompting was also a bit of an issue. Our first method was to feed the API the first 30 lines of the transcript of interest and have it generate a question based on the roadmap Will Styler tends to give at the beginning of his lectures. However, GPT-3.5 struggled greatly with this, and never elaborated on the concepts given, but only asked a question based on what was explicitly said. This lead to questions like 'What is the title of Lecture 1?' which were not helpful for student understanding and learning. GPT-4 was more capable of handling this style of prompting, so we left the function in the code in case a user would like to experiment with it and GPT-4. GPT's he responses are not always uniform, so it will occasionally give a question in the wrong format which our code is not equipped to handle, and we have to throw the question out. In addition, there are some cases where the API generates a question that assumes the user knows what lecture and the contents of the excerpt eben though the prompt explicitly states that they do not.

&ensp; &ensp; &ensp; We wanted to have short answer questions as a part of this app's repertoire, but API's output of this question type varies vastly. Sometimes it is a reasonable fill in the blank, and others it is not. In addition, validating the answer is tough, as we only accept exact matches to GPT. One solution we were exploring was passing back in the user input and question to GPT and prompting if the user is correct to determine validity of the answer, however, it is still a work in progress at this time. We also wanted difficulty as a feature of the app, but after implementation, it was hard for us to test as we've never taken LIGN 101 and the difficulty of a question is subjective. We trust that GPT is following the difficulty instruction.

&ensp; &ensp; &ensp; Moving forward some things to keep in mind is the developer's ability to customize the interface without being restricted by a 3rd party interface. Incorporating JavaScript, HTML, and CSS could ultimately aid in the overall aesthetics of the application. Aside from aesthetics, we could have a better implementation of the overall API calls to minimize incorrectly generated questions, perhaps through more detailed prompting and error handling cases where the API will continuously retry until the problem is fixed. We could also improve the question quality by using the pdf problem sets associated with each lecture, or the final exam review guide to serve as additional context for GPT and guide it to creating a high quality question. 
